# -*- text -*-
#
# Copyright (c) 2021-2022 Nanook Consulting.  All rights reserved.
# Copyright (c) 2022      IBM Corporation.  All rights reserved.
# $COPYRIGHT$
#
# Additional copyrights may follow
#
# $HEADER$
#
#
#
[version]
%s (%s) %s

%s
#
[usage]
%s (%s) %s

Usage: %s [OPTION]...
Submit job for execution

The following list of command line options are available. Note that
more detailed help for any option can be obtained by adding that
option to the help request as "--help <option>".

/*****      General Options      *****/

-h|--help                            Display help information and exit.
-h|--help <arg0>                     Help for the specified option

/*****      Placement Options      *****/

 -m|--distribution <arg0>            Specify alternate distribution methods for
                                     remote processes
-c|--cpus-per-task <arg0>            Request that ncpus be allocated per process

Report bugs to %s
#
[distribution]
Specify alternate distribution methods for remote processes. This option controls the
distribution of tasks to the nodes on which resources have been allocated, and the
distribution of those resources to tasks for binding (task affinity). The first
distribution method (before the first ":") controls the distribution of tasks to nodes.
The second distribution method (after the first ":") controls the distribution of
allocated CPUs across sockets for binding to tasks. The third distribution method (after
the second ":") controls the distribution of allocated CPUs across cores for binding to
tasks. The second and third distributions apply only if task affinity is enabled. The third
distribution is supported only if the task/cgroup plugin is configured. The default value
for each distribution type is specified by *.

Note that with select/cons_res, the number of CPUs allocated on each socket and node may be
different. Refer to http://slurm.schedmd.com/mc_support.html for more information on
resource allocation, distribution of tasks to nodes, and binding of tasks to CPUs.

First distribution method (distribution of tasks across nodes):

    *
        Use the default method for distributing tasks to nodes (block).
    block
        The block distribution method will distribute tasks to a node such that consecutive
        tasks share a node. For example, consider an allocation of three nodes each with
        two cpus. A four-task block distribution request will distribute those tasks to the
        nodes with tasks one and two on the first node, task three on the second node, and
        task four on the third node. Block distribution is the default behavior if the
        number of tasks exceeds the number of allocated nodes.
    cyclic
        The cyclic distribution method will distribute tasks to a node such that
        consecutive tasks are distributed over consecutive nodes (in a round-robin
        fashion). For example, consider an allocation of three nodes each with two cpus. A
        four-task cyclic distribution request will distribute those tasks to the nodes with
        tasks one and four on the first node, task two on the second node, and task three
        on the third node. Note that when SelectType is select/cons_res, the same number of
        CPUs may not be allocated on each node. Task distribution will be round-robin among
        all the nodes with CPUs yet to be assigned to tasks. Cyclic distribution is the
        default behavior if the number of tasks is no larger than the number of allocated
        nodes.
    plane
        The tasks are distributed in blocks of a specified size. The options include a
        number representing the size of the task block. This is followed by an optional
        specification of the task distribution scheme within a block of tasks and between
        the blocks of tasks. The number of tasks distributed to each node is the same as
        for cyclic distribution, but the taskids assigned to each node depend on the plane
        size. For more details (including examples and diagrams), please see
        http://slurm.schedmd.com/mc_support.html
        and
        http://slurm.schedmd.com/dist_plane.html
    arbitrary
        The arbitrary method of distribution will allocate processes in-order as listed in
        file designated by the environment variable SLURM_HOSTFILE. If this variable is
        listed it will over ride any other method specified. If not set the method will
        default to block. Inside the hostfile must contain at minimum the number of hosts
        requested and be one per line or comma separated. If specifying a task count (-n,
        --ntasks=<number>), your tasks will be laid out on the nodes in the order of the
        file.
        NOTE: The arbitrary distribution option on a job allocation only controls the nodes
        to be allocated to the job and not the allocation of CPUs on those nodes. This
        option is meant primarily to control a job step's task layout in an existing job
        allocation for the srun command.

    Second distribution method (distribution of CPUs across sockets for binding):

    *
        Use the default method for distributing CPUs across sockets (cyclic).
    block
        The block distribution method will distribute allocated CPUs consecutively from the
        same socket for binding to tasks, before using the next consecutive socket.
    cyclic
        The cyclic distribution method will distribute allocated CPUs for binding to a
        given task consecutively from the same socket, and from the next consecutive socket
        for the next task, in a round-robin fashion across sockets.
    fcyclic
        The fcyclic distribution method will distribute allocated CPUs for binding to tasks
        from consecutive sockets in a round-robin fashion across the sockets.

    Third distribution method (distribution of CPUs across cores for binding):

    *
        Use the default method for distributing CPUs across cores (inherited from second
        distribution method).
    block
        The block distribution method will distribute allocated CPUs consecutively from the
        same core for binding to tasks, before using the next consecutive core.
    cyclic
        The cyclic distribution method will distribute allocated CPUs for binding to a
        given task consecutively from the same core, and from the next consecutive core for
        the next task, in a round-robin fashion across cores.
    fcyclic
        The fcyclic distribution method will distribute allocated CPUs for binding to tasks
        from consecutive cores in a round-robin fashion across the cores.

    Optional control for task distribution over nodes:

    Pack
        Rather than evenly distributing a job step's tasks evenly across it's allocated
        nodes, pack them as tightly as possible on the nodes.
    NoPack
        Rather than packing a job step's tasks as tightly as possible on the nodes,
        distribute them evenly. This user option will supersede the SelectTypeParameters
        CR_Pack_Nodes configuration parameter.
#
[cpu_bind]
Bind tasks to CPUs. Used only when the task/affinity or task/cgroup plugin is enabled.
NOTE: To have Slurm always report on the selected CPU binding for all commands executed in
a shell, you can enable verbose mode by setting the SLURM_CPU_BIND environment variable
value to "verbose".

The following informational environment variables are set when --cpu_bind is in use:

        SLURM_CPU_BIND_VERBOSE
        SLURM_CPU_BIND_TYPE
        SLURM_CPU_BIND_LIST

See the ENVIRONMENT VARIABLES section for a more detailed description of the individual
SLURM_CPU_BIND variables. These variable are available only if the task/affinity plugin is
configured.

When using --cpus-per-task to run multithreaded tasks, be aware that CPU binding is
inherited from the parent of the process. This means that the multithreaded task should
either specify or clear the CPU binding itself to avoid having all threads of the
multithreaded task use the same mask/CPU as the parent. Alternatively, fat masks (masks
which specify more than one allowed CPU) could be used for the tasks in order to provide
multiple CPUs for the multithreaded tasks.

By default, a job step has access to every CPU allocated to the job. To ensure that
distinct CPUs are allocated to each job step, use the --exclusive option.

Note that a job step can be allocated different numbers of CPUs on each node or be
allocated CPUs not starting at location zero. Therefore one of the options which
automatically generate the task binding is recommended. Explicitly specified masks or
bindings are only honored when the job step has been allocated every available CPU on the
node.

Binding a task to a NUMA locality domain means to bind the task to the set of CPUs that
belong to the NUMA locality domain or "NUMA node". If NUMA locality domain options are used
on systems with no NUMA support, then each socket is considered a locality domain.

    Auto Binding
        Applies only when task/affinity is enabled. If the job step allocation includes an
        allocation with a number of sockets, cores, or threads equal to the number of tasks
        times cpus-per-task, then the tasks will by default be bound to the appropriate
        resources (auto binding). Disable this mode of operation by explicitly setting
        "--cpu_bind=none". Use TaskPluginParam=autobind=[threads|cores|sockets] to set a
        default cpu binding in case "auto binding" doesn't find a match.

    Supported options include:

        q[uiet]
            Quietly bind before task runs (default)
        v[erbose]
            Verbosely report binding before task runs
        no[ne]
            Do not bind tasks to CPUs (default unless auto binding is applied)
        rank
            Automatically bind by task rank. The lowest numbered task on each node is bound
            to socket (or core or thread) zero, etc. Not supported unless the entire node
            is allocated to the job.
        map_cpu:<list>
            Bind by mapping CPU IDs to tasks as specified where <list> is
            <cpuid1>,<cpuid2>,...<cpuidN>. The mapping is specified for a node and
            identical mapping is applied to the tasks on every node (i.e. the lowest task
            ID on each node is mapped to the first CPU ID specified in the list, etc.). CPU
            IDs are interpreted as decimal values unless they are preceded with '0x' in
            which case they are interpreted as hexadecimal values. Not supported unless the
            entire node is allocated to the job.
        mask_cpu:<list>
            Bind by setting CPU masks on tasks as specified where <list> is
            <mask1>,<mask2>,...<maskN>. The mapping is specified for a node and identical
            mapping is applied to the tasks on every node (i.e. the lowest task ID on each
            node is mapped to the first mask specified in the list, etc.). CPU masks are
            always interpreted as hexadecimal values but can be preceded with an optional
            '0x'. Not supported unless the entire node is allocated to the job.
        rank_ldom
            Bind to a NUMA locality domain by rank. Not supported unless the entire node is
            allocated to the job.
        map_ldom:<list>
            Bind by mapping NUMA locality domain IDs to tasks as specified where <list> is
            <ldom1>,<ldom2>,...<ldomN>. The locality domain IDs are interpreted as decimal
            values unless they are preceded with '0x' in which case they are interpreted as
            hexadecimal values. Not supported unless the entire node is allocated to the
            job.
        mask_ldom:<list>
            Bind by setting NUMA locality domain masks on tasks as specified where <list>
            is <mask1>,<mask2>,...<maskN>. NUMA locality domain masks are always
            interpreted as hexadecimal values but can be preceded with an optional '0x'.
            Not supported unless the entire node is allocated to the job.
        sockets
            Automatically generate masks binding tasks to sockets. Only the CPUs on the
            socket which have been allocated to the job will be used. If the number of
            tasks differs from the number of allocated sockets this can result in
            sub-optimal binding.
        cores
            Automatically generate masks binding tasks to cores. If the number of tasks
            differs from the number of allocated cores this can result in sub-optimal
            binding.
        threads
            Automatically generate masks binding tasks to threads. If the number of tasks
            differs from the number of allocated threads this can result in sub-optimal
            binding.
        ldoms
            Automatically generate masks binding tasks to NUMA locality domains. If the
            number of tasks differs from the number of allocated locality domains this can
            result in sub-optimal binding.
        boards
            Automatically generate masks binding tasks to boards. If the number of tasks
            differs from the number of allocated boards this can result in sub-optimal
            binding. This option is supported by the task/cgroup plugin only.
        help
            Show help message for cpu_bind
#
[cpus-per-task]
Request that ncpus be allocated per process. This may be useful if the job is multithreaded
and requires more than one CPU per task for optimal performance. The default is one CPU per
process. If -c is specified without -n, as many tasks will be allocated per node as
possible while satisfying the -c restriction. For instance on a cluster with 8 CPUs per
node, a job request for 4 nodes and 3 CPUs per task may be allocated 3 or 6 CPUs per node
(1 or 2 tasks per node) depending upon resource consumption by other jobs. Such a job may
be unable to execute more than a total of 4 tasks. This option may also be useful to spawn
tasks without allocating resources to the job step from the job's allocation when running
multiple job steps with the --exclusive option.

WARNING: There are configurations and options interpreted differently by job and job step
requests which can result in inconsistencies for this option. For example srun -c2
--threads-per-core=1 prog may allocate two cores for the job, but if each of those cores
contains two threads, the job allocation will include four CPUs. The job step allocation
will then launch two threads per CPU for a total of two tasks.

WARNING: When srun is executed from within salloc or sbatch, there are configurations and
options which can result in inconsistent allocations when -c has a value greater than -c on
salloc or sbatch.
#

